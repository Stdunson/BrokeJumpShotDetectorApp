{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec87504",
   "metadata": {},
   "source": [
    "Jupyter Notebook for Training of Broke Jumpshot Detector ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17640d2d",
   "metadata": {},
   "source": [
    "Steps: \n",
    "1. Imports\n",
    "2. Add model for transfer learning\n",
    "3. Load dataset\n",
    "4. Define model architecture\n",
    "5. Split into train, validation, and test\n",
    "6. Train\n",
    "7. Test Performance\n",
    "8. Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a70fa",
   "metadata": {},
   "source": [
    "Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4a22cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e36a9",
   "metadata": {},
   "source": [
    "Step 2: Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024c85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Transfer Learning Model\n",
    "#Mediapipe Pose(blazepose), will extract keypoints from images, which will be used to classify shots as broke or not broke\n",
    "#Movenet is what we're using, but it;s within mediapipe\n",
    "import mediapipe as mp\n",
    "mp_pose = mp.solutions.pose\n",
    "#To extract keypoints: mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17350761",
   "metadata": {},
   "source": [
    "Step 3: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595ce4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 220\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Load dataset\n",
    "datasetPath = \"./dataset/\"\n",
    "imageData = []\n",
    "phases = [\"followthrough\", \"setpoint\", \"shotpocket\"]\n",
    "labels = [\"broke\", \"butter\"]\n",
    "\n",
    "for phase in phases:\n",
    "    for label in labels:\n",
    "        path = os.path.join(datasetPath, phase, label)\n",
    "\n",
    "        images = glob(os.path.join(path, \"*.jpg\")) + glob(os.path.join(path, \"*.png\")) + glob(os.path.join(path, \"*.jpeg\"))\n",
    "\n",
    "\n",
    "        for imgFile in images:\n",
    "            imageData.append((imgFile, phase, label))\n",
    "\n",
    "print(f\"Total images loaded: {len(imageData)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34c80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize keypoints function\n",
    "def normalize(keypoints):\n",
    "    \n",
    "    xs = [kp[0] for kp in keypoints]\n",
    "    ys = [kp[1] for kp in keypoints]\n",
    "\n",
    "    min_x, max_x = min(xs), max(xs)\n",
    "    min_y, max_y = min(ys), max(ys)\n",
    "\n",
    "    width = max_x - min_x\n",
    "    height = max_y - min_y\n",
    "\n",
    "    return [((x-min_x)/width, (y-min_y)/height, z, v)\n",
    "            for x,y,z,v in keypoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c0e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1920, 1080, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764095068.705624 13485602 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1764095068.793787 13488050 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764095068.819683 13488050 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/ShavaughnDunson/Documents/Code/BrokeJumpShotDetectorApp/brokeShotEnv/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "No pose detected\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (800, 640, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1280, 720, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 410, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (300, 168, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "No pose detected\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 407, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 368, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 407, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (457, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "No pose detected\n",
      "Shape: (612, 408, 3)\n",
      "No pose detected\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (399, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (292, 212, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (460, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (960, 536, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 406, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (265, 316, 3)\n",
      "No pose detected\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 369, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 406, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 410, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 437, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1200, 1200, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (960, 536, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (292, 212, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 406, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1280, 720, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (800, 640, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "No pose detected\n",
      "Shape: (612, 435, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "No pose detected\n",
      "Shape: (612, 407, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (431, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 399, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (421, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (407, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 409, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 406, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 407, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 409, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 407, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (562, 171, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (612, 408, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (408, 612, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1280, 720, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (800, 640, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (292, 212, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1080, 1920, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (960, 536, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n",
      "Shape: (1920, 1080, 3)\n",
      "Detected 33 landmarks\n"
     ]
    }
   ],
   "source": [
    "#Extract pose data and save in a list\n",
    "#Each image has its pose data, class(set point, etc), and brokeness\n",
    "#For each image, extract pose, normalize pose, infer label from filename, append to list\n",
    "poseData = [] #holds keypoints, phase, label\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=2, min_detection_confidence=0.5)\n",
    "\n",
    "for i in range(len(imageData)):\n",
    "    imgFile, phase, label = imageData[i]\n",
    "\n",
    "    img = cv2.imread(imgFile)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    print(\"Shape:\", img_rgb.shape)\n",
    "\n",
    "    results = pose.process(img_rgb)\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        print(\"No pose detected\")\n",
    "    else:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        print(\"Detected\", len(landmarks), \"landmarks\")\n",
    "\n",
    "    keypoints = []\n",
    "    for landmark in landmarks:\n",
    "        keypoints.append((landmark.x, landmark.y, landmark.z, landmark.visibility))\n",
    "    \n",
    "    poseData.append({\n",
    "        \"keypoints\": normalize(keypoints), \n",
    "        \"phase\": phase, \n",
    "        \"label\": label,\n",
    "        \"path\": imgFile\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52940592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to pytorch dataset\n",
    "labelToIdx = {\"broke\": 0, \"butter\": 1}\n",
    "phaseToIdx = {\"shotpocket\": 0, \"setpoint\": 1, \"followthrough\": 2}\n",
    "\n",
    "class PoseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        poseVector = np.array(item[\"keypoints\"], dtype=np.float32).flatten()\n",
    "\n",
    "        phaseVector = torch.zeros(len(phaseToIdx), dtype=torch.float32)\n",
    "        phase_idx = phaseToIdx[item[\"phase\"]]\n",
    "\n",
    "        #Pose + phase vector\n",
    "        inputVector = torch.tensor(poseVector, dtype=torch.float32)\n",
    "        inputVector = torch.cat([inputVector, phaseVector])\n",
    "\n",
    "        label_idx = labelToIdx[item[\"label\"]]\n",
    "\n",
    "        return inputVector, torch.tensor(label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f3f6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize dataset\n",
    "dataset = PoseDataset(poseData)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "batchInputs, batchLabels = batch\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def show_pose_on_image(image_path, phase=None, label=None):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = pose.process(img_rgb)\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        print(\"No pose detected in:\", image_path)\n",
    "        return\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        img_rgb,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(6,8))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if label and phase:\n",
    "        print(f\"Label: {label}, Phase: {phase}\")\n",
    "    else:\n",
    "        print(f\"Image path: {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f4039",
   "metadata": {},
   "source": [
    "Step 4: Define model architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f25ef8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Define model architecture\n",
    "#MLP model to classify pose keypoints\n",
    "class PoseMLP(nn.Module):\n",
    "    def __init__(self, input_dim = 135, hidden_dim1 = 128, hidden_dim2 = 64, dropout = 0.2, output_dim = 1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.BatchNorm1d(hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.BatchNorm1d(hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "568ec8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Model\n",
    "model = PoseMLP(input_dim=135, hidden_dim1=128, hidden_dim2=64, dropout=0.2, output_dim=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d325cb6",
   "metadata": {},
   "source": [
    "Step 5: Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "640c4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Split data within list into train, validation, and test\n",
    "train_data, test_data = train_test_split(poseData, test_size=0.1, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2/.9, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c22b94",
   "metadata": {},
   "source": [
    "Step 6: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ee3d77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 135])\n",
      "Labels shape: torch.Size([16])\n",
      "Epoch 1/20, Train Loss: 0.7172, Val Loss: 0.6803\n",
      "Epoch 2/20, Train Loss: 0.6755, Val Loss: 0.6699\n",
      "Epoch 3/20, Train Loss: 0.5960, Val Loss: 0.6668\n",
      "Epoch 4/20, Train Loss: 0.7981, Val Loss: 0.6700\n",
      "Epoch 5/20, Train Loss: 0.6875, Val Loss: 0.6922\n",
      "Epoch 6/20, Train Loss: 0.5453, Val Loss: 0.7196\n",
      "Epoch 7/20, Train Loss: 0.7250, Val Loss: 0.7683\n",
      "Epoch 8/20, Train Loss: 0.5721, Val Loss: 0.8184\n",
      "Epoch 9/20, Train Loss: 0.4845, Val Loss: 0.8143\n",
      "Epoch 10/20, Train Loss: 0.5005, Val Loss: 0.8106\n",
      "Epoch 11/20, Train Loss: 0.4869, Val Loss: 0.8209\n",
      "Epoch 12/20, Train Loss: 0.4924, Val Loss: 0.8341\n",
      "Epoch 13/20, Train Loss: 0.5431, Val Loss: 0.9294\n",
      "Epoch 14/20, Train Loss: 0.4776, Val Loss: 0.9868\n",
      "Epoch 15/20, Train Loss: 0.4461, Val Loss: 0.9131\n",
      "Epoch 16/20, Train Loss: 0.3407, Val Loss: 0.9069\n",
      "Epoch 17/20, Train Loss: 0.3193, Val Loss: 0.9502\n",
      "Epoch 18/20, Train Loss: 0.3736, Val Loss: 1.0118\n",
      "Epoch 19/20, Train Loss: 0.6812, Val Loss: 1.0551\n",
      "Epoch 20/20, Train Loss: 0.3738, Val Loss: 0.9508\n"
     ]
    }
   ],
   "source": [
    "#Step 6: Train\n",
    "numEpochs = 20\n",
    "trainSet = PoseDataset(train_data)\n",
    "trainLoader = DataLoader(trainSet, batch_size=16, shuffle=True)\n",
    "valSet = PoseDataset(val_data)\n",
    "valLoader = DataLoader(valSet, batch_size=16, shuffle=False)\n",
    "\n",
    "for batch in trainLoader:\n",
    "    inputs, labels = batch\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break\n",
    "\n",
    "#GPU support\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(numEpochs):\n",
    "    model.train()\n",
    "    for inputs, labels in trainLoader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #Training step\n",
    "        predictions = model(inputs)\n",
    "        loss = criterion(predictions, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #Validation step\n",
    "    model.eval()\n",
    "    valLoss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for valInputs, valLabels in valLoader:\n",
    "            valInputs = valInputs.to(device)\n",
    "            valLabels = valLabels.to(device)\n",
    "            valPredictions = model(valInputs).squeeze()\n",
    "            valLoss += criterion(valPredictions, valLabels.float()).item()\n",
    "    valLoss /= len(valLoader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{numEpochs}, Train Loss: {loss.item():.4f}, Val Loss: {valLoss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63710ad",
   "metadata": {},
   "source": [
    "Step 7: Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c72a95dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.09090909090909%\n",
      "Confusion Matrix:\n",
      "[[8 4]\n",
      " [5 5]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       broke       0.62      0.67      0.64        12\n",
      "      butter       0.56      0.50      0.53        10\n",
      "\n",
      "    accuracy                           0.59        22\n",
      "   macro avg       0.59      0.58      0.58        22\n",
      "weighted avg       0.59      0.59      0.59        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 7: Test Performance\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "testSet = PoseDataset(test_data)\n",
    "testLoader = DataLoader(testSet, batch_size=32, shuffle=False)\n",
    "\n",
    "allLabels = []\n",
    "allPredictions = []\n",
    "allPhases = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for poses, labels in testLoader:\n",
    "        poses = poses.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        predictions = model(poses).squeeze()\n",
    "        predictedLabels = (torch.sigmoid(predictions) > 0.5).int()\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predictedLabels == labels).sum().item()\n",
    "\n",
    "        allLabels.extend(labels.cpu().numpy())\n",
    "        allPredictions.extend(predictedLabels.cpu().numpy())\n",
    "\n",
    "\n",
    "accuracyScore = accuracy_score(allLabels, allPredictions)\n",
    "print(f\"Test Accuracy: {accuracyScore * 100}%\")\n",
    "\n",
    "cm = confusion_matrix(allLabels, allPredictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(allLabels, allPredictions, target_names=[\"broke\", \"butter\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize predictions/results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93f79c",
   "metadata": {},
   "source": [
    "Step 8: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ceaf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model should be able to take in a picture and output its prediction on whether the pose is broke or not broke\n",
    "#Full Model Save\n",
    "modelVersion = \"v1\" #11/25/25 1:00pm, update when retrained\n",
    "os.makedirs(\"MLmodels\", exist_ok=True)\n",
    "modelName = \"broke_jump_shot_detector_model_\" + modelVersion + \".pth\"\n",
    "modelPath = os.path.join(\"./MLmodels/\", modelName)\n",
    "torch.save(model, modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ac8eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights Save\n",
    "weightVersion = \"v1\" #11/25/25 1:00pm, update when retrained\n",
    "os.makedirs(\"MLweights\", exist_ok=True)\n",
    "weightsName = \"broke_jump_shot_detector_weights_\" + weightVersion + \".pth\"\n",
    "weightsPath = os.path.join(\"./MLweights/\", weightsName)\n",
    "torch.save(model.state_dict(), weightsPath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brokeShotEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
